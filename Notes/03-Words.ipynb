{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words, Tokens, Stems, Lemmas\n",
    "\n",
    "## and the NLTK\n",
    "\n",
    "The NLTK is the Python Natural Language (processing) ToolKit. To use it, we import the package like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to importing the NLTK, we also need to make sure to download the language models for English. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('book')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, make sure that we're in the directory where `moonstone.md` exists, or else we won't be able to load the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jon/Code/course-computational-literary-analysis\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will likely be different on your system, so don't just run this command blindly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mHomework\u001b[0m/  \u001b[01;34mHW1\u001b[0m/  LICENSE  moonstone.md  \u001b[01;34mNotes\u001b[0m/  README.md\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: as someone pointed out in the course chatroom, sometimes Windows doesn't load the file as unicode by default, so we have to tell it to do this explicitly: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "moonstoneRaw = open('moonstone.md', encoding=\"UTF-8\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I've prepared my text ahead of time by marking the beginnings and ends of the Betteredge and Clack sections with `%%%%%`. (This is an arbitrary mark, and doesn't really mean anything.) This allows me to split the text like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "moonstoneParts = moonstoneRaw.split('%%%%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(moonstoneParts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the second part look like? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "### Chapter I\n",
      "\n",
      "In the first part of ROBINSON CRUSOE, at page one hundred and\n",
      "twenty-nine, you will find it thus written:\n",
      "\n",
      "“Now I saw, though too late, the Folly of beginning a Work before we\n",
      "count the Cost, and before we judge rightly of our own Strength to go\n",
      "through with it.”\n",
      "\n",
      "Only yesterday, I opened my ROBINSON CRUSOE at that place. Only this\n",
      "morning (May twenty-first, Eighteen hundred and fifty), came my lady’s\n",
      "nephew, Mr. Franklin Blake, and held a short conversation with me, as\n",
      "follows:\n"
     ]
    }
   ],
   "source": [
    "print(moonstoneParts[1][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, that's Betteredge. Now how about the fourth part? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "### Chapter I\n",
      "\n",
      "I am indebted to my dear parents (both now in heaven) for having had\n",
      "habits of order and regularity instilled into me at a very early age.\n",
      "\n",
      "In that happy bygone time, I was taught to keep my hair tidy at all\n",
      "hours of the day and night, and to fold up every article of my clothing\n",
      "carefully, in the same order, on the same chair, in the same place at\n",
      "the foot of the bed, before retiring to rest. An entry of the day’s\n",
      "events in my little diary invariably preceded the folding up. Th\n"
     ]
    }
   ],
   "source": [
    "print(moonstoneParts[3][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's certainly Miss Clack. Let's assign these both to variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "betteredge = moonstoneParts[1]\n",
    "clack = moonstoneParts[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens and Tokenizing\n",
    "\n",
    "Tokens are word-like objects. Punctuation marks and parts of words, like \"ca\" and \"n't\" are also considered tokens by some tokenizers. Let's make a test sentence, and try to tokenize it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "testSentence = \"\"\"\n",
    "I am indebted to my dear parents (both now in heaven) \n",
    "for having had habits of order and regularity \n",
    "instilled into me at a very early age.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I am indebted to my dear parents (both now in heaven) \n",
      "for having had habits of order and regularity \n",
      "instilled into me at a very early age.\n"
     ]
    }
   ],
   "source": [
    "print(testSentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the nltk function `word_tokenize()`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'indebted',\n",
       " 'to',\n",
       " 'my',\n",
       " 'dear',\n",
       " 'parents',\n",
       " '(',\n",
       " 'both',\n",
       " 'now',\n",
       " 'in',\n",
       " 'heaven',\n",
       " ')',\n",
       " 'for',\n",
       " 'having',\n",
       " 'had',\n",
       " 'habits',\n",
       " 'of',\n",
       " 'order',\n",
       " 'and',\n",
       " 'regularity',\n",
       " 'instilled',\n",
       " 'into',\n",
       " 'me',\n",
       " 'at',\n",
       " 'a',\n",
       " 'very',\n",
       " 'early',\n",
       " 'age',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(testSentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many tokens did it find? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk.word_tokenize(testSentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try another sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An',\n",
       " 'entry',\n",
       " 'of',\n",
       " 'the',\n",
       " 'day',\n",
       " '’',\n",
       " 's',\n",
       " 'events',\n",
       " 'in',\n",
       " 'my',\n",
       " 'little',\n",
       " 'diary',\n",
       " 'invariably',\n",
       " 'preceded',\n",
       " 'the',\n",
       " 'folding',\n",
       " 'up',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(\"An entry of the day’s events in my little diary invariably preceded the folding up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice what happens there with \"day's\"? What if our sentence contains a contraction? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'ca', \"n't\", 'believe', 'this', '!']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(\"I can't believe this!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stems and Stemming\n",
    "\n",
    "To stem a word, we first have to instantiate, or make a fresh copy of, our semmer object: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test it on three different forms of the same stem: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "believ\n",
      "believ\n",
      "believ\n"
     ]
    }
   ],
   "source": [
    "for word in [\"believe\", \"belief\", \"believing\"]:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'believ'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"believe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmas and Lemmatizers\n",
    "\n",
    "A lemma is the \"dictionary form\" of a word, so the lemma for \"jumps\" is \"jump.\" Lemmatizing often doesn't transform the text as much as stemming. First, instantiate the lemmatizer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'believe'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"believe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "believe\n",
      "belief\n",
      "believing\n"
     ]
    }
   ],
   "source": [
    "for word in [\"believe\", \"belief\", \"believing\"]:\n",
    "    print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy\n",
      "happier\n",
      "happiest\n"
     ]
    }
   ],
   "source": [
    "for word in [\"happy\", \"happier\", \"happiest\"]:\n",
    "    print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump\n",
      "jumping\n",
      "jump\n"
     ]
    }
   ],
   "source": [
    "for word in [\"jumps\", \"jumping\", \"jump\"]:\n",
    "    print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "testTokens = nltk.word_tokenize(testSentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application: Comparing Miss Clack with Betteredge \n",
    "\n",
    "First, tokenize each text: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "clackTokens = nltk.word_tokenize(clack)\n",
    "betteredgeTokens = nltk.word_tokenize(betteredge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare the lengths of each: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36247, 94899)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clackTokens), len(betteredgeTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert each token into its stem: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clackStems = []\n",
    "for word in clackTokens: \n",
    "    stem = stemmer.stem(word)\n",
    "    clackStems.append(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "betteredgeStems = []\n",
    "for word in betteredgeTokens: \n",
    "    stem = stemmer.stem(word)\n",
    "    betteredgeStems.append(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a word frequency table for each of these collections of stems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clackStemsDict = {}\n",
    "for stem in clackStems:\n",
    "    # If our stem is not already in the dictionary, \n",
    "    # it has a frequency of one. \n",
    "    if stem not in clackStemsDict: \n",
    "        clackStemsDict[stem] = 1\n",
    "    else: \n",
    "        # Otherwise, increase the count by one. \n",
    "        clackStemsDict[stem] = clackStemsDict[stem] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "betteredgeStemsDict = {}\n",
    "for stem in betteredgeStems:\n",
    "    # If our stem is not already in the dictionary, \n",
    "    # it has a frequency of one. \n",
    "    if stem not in betteredgeStemsDict: \n",
    "        betteredgeStemsDict[stem] = 1\n",
    "    else: \n",
    "        betteredgeStemsDict[stem] = betteredgeStemsDict[stem] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36247, 94899)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clackTokens), len(betteredgeTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the proportions of exclamation marks used by each. We're dividing by the total number of tokens in each, so that we're dealing with proportions, rather than raw counts: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0068419455403205785\n",
      "0.003530068809997998\n"
     ]
    }
   ],
   "source": [
    "print(clackStemsDict['!'] / len(clackTokens)) \n",
    "print(betteredgeStemsDict['!'] / len(betteredgeTokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like Miss Clack uses exclamation point (!) about twice as much as Betteredge!!!\n",
    "\n",
    "## Sentence Tokenization\n",
    "\n",
    "We can also tokenize by sentences instead of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "betteredgeSents = nltk.sent_tokenize(betteredge)\n",
    "clackSents = nltk.sent_tokenize(clack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' \\n\\n### Chapter I\\n\\nI am indebted to my dear parents (both now in heaven) for having had\\nhabits of order and regularity instilled into me at a very early age.',\n",
       " 'In that happy bygone time, I was taught to keep my hair tidy at all\\nhours of the day and night, and to fold up every article of my clothing\\ncarefully, in the same order, on the same chair, in the same place at\\nthe foot of the bed, before retiring to rest.',\n",
       " 'An entry of the day’s\\nevents in my little diary invariably preceded the folding up.',\n",
       " 'The\\n“Evening Hymn” (repeated in bed) invariably followed the folding up.',\n",
       " 'And the sweet sleep of childhood invariably followed the “Evening Hymn.”\\n\\nIn later life (alas!)']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clackSents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what some lengths of some sentences are: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "100\n",
      "279\n",
      "80\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "for sent in betteredgeSents[100:105]: \n",
    "    print(len(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and we can build up lists of sentence lengths for each character: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "clackSentenceLengths = []\n",
    "for sent in clackSents: \n",
    "    clackSentenceLengths.append(len(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "betteredgeSentenceLengths = []\n",
    "for sent in betteredgeSents: \n",
    "    betteredgeSentenceLengths.append(len(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can find the average sentence length for each: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.87649164677805"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(clackSentenceLengths)/len(clackSentenceLengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112.04363827549948"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(betteredgeSentenceLengths)/len(betteredgeSentenceLengths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
